agentOpts = rlDDPGAgentOptions(...
    "SampleTime",sampleTime,...
    "TargetSmoothFactor",1e-3,...
    "DiscountFactor",0.995, ...
    "MiniBatchSize",128, ...
    "ExperienceBufferLength",1e6); 

agentOpts.NoiseOptions.Variance = 0.1;
agentOpts.NoiseOptions.VarianceDecayRate = 1e-5;

obstacleAvoidanceAgent1 = rlDDPGAgent(actor1,critic1,agentOpts);
obstacleAvoidanceAgent2 = rlDDPGAgent(actor2,critic2,agentOpts);
%%
maxEpisodes = 10000;
maxSteps = ceil(Tfinal/sampleTime);
trainOpts = rlTrainingOptions(...
    "MaxEpisodes",maxEpisodes, ...
    "MaxStepsPerEpisode",maxSteps, ...
    "ScoreAveragingWindowLength",50, ...
    "StopTrainingCriteria","AverageReward", ...
    "StopTrainingValue",550, ...
    "SaveAgentCriteria", "EpisodeReward", ...
    "SaveAgentValue", 350, ...
    'SaveAgentDirectory', pwd + "\multi2\Agents", ...
    "Verbose", true, ...
    "Plots","training-progress");

trainingStats = train([obstacleAvoidanceAgent1, obstacleAvoidanceAgent2],env,trainOpts);

